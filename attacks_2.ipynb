{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model-based attacks VS FGSM / iFGSM"
      ],
      "metadata": {
        "id": "kgeuhz3EgsmA"
      },
      "id": "kgeuhz3EgsmA"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "lYEegFEd-Ta4",
      "metadata": {
        "id": "lYEegFEd-Ta4"
      },
      "outputs": [],
      "source": [
        "SEED = 123\n",
        "import random, os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5698e565",
      "metadata": {
        "id": "5698e565"
      },
      "source": [
        "### EarlyStopping utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4c8d9acc",
      "metadata": {
        "id": "4c8d9acc"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=1e-4, mode='min'):\n",
        "        assert mode in ('min', 'max')\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.mode = mode\n",
        "        self.best = None\n",
        "        self.counter = 0\n",
        "        self.should_stop = False\n",
        "\n",
        "    def step(self, metric):\n",
        "        if self.best is None:\n",
        "            self.best = metric\n",
        "            self.counter = 0\n",
        "            return False\n",
        "        improvement = (metric < self.best - self.min_delta) if self.mode == 'min' else (metric > self.best + self.min_delta)\n",
        "        if improvement:\n",
        "            self.best = metric\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.should_stop = True\n",
        "        return self.should_stop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "bLFzVkZkg-f9"
      },
      "id": "bLFzVkZkg-f9"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "86376f21",
      "metadata": {
        "id": "86376f21"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "09da18d3",
      "metadata": {
        "id": "09da18d3"
      },
      "outputs": [],
      "source": [
        "TRAIN_PATH = Path('PowerCons_TRAIN.tsv')\n",
        "TEST_PATH = Path('PowerCons_TEST.tsv')\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 50\n",
        "LR = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "96b46b88",
      "metadata": {
        "id": "96b46b88"
      },
      "outputs": [],
      "source": [
        "def load_powercon(path: Path):\n",
        "    df = pd.read_csv(path, sep='\\t', header=None)\n",
        "    y = df.iloc[:, 0].values\n",
        "    X = df.iloc[:, 1:].values.astype(np.float32)\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y).astype(np.int64)\n",
        "    return X, y_encoded, le.classes_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "faaef73d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faaef73d",
        "outputId": "742d3128-83e7-4eeb-90b6-d7a2247073d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (180, 144, 1) Test shape: (180, 144, 1) n_classes: 2\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train, classes_ = load_powercon(TRAIN_PATH)\n",
        "X_test, y_test, _ = load_powercon(TEST_PATH)\n",
        "n_classes = len(classes_)\n",
        "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape, 'n_classes:', n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b6952fde",
      "metadata": {
        "id": "b6952fde"
      },
      "outputs": [],
      "source": [
        "class PowerConDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_ds = PowerConDataset(X_train, y_train)\n",
        "test_ds = PowerConDataset(X_test, y_test)\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "evY2hOVW-YwX",
      "metadata": {
        "id": "evY2hOVW-YwX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a265c0af-aaef-4b85-9334-f6e10023d28c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "def seed_worker(worker_id):\n",
        "    worker_seed = SEED + worker_id\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(SEED)\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTMClassifier"
      ],
      "metadata": {
        "id": "2xywORKmhG9M"
      },
      "id": "2xywORKmhG9M"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "bff920e8",
      "metadata": {
        "id": "bff920e8"
      },
      "outputs": [],
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, n_classes, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, n_classes)\n",
        "    def forward(self, x):\n",
        "        out, (h_n, _) = self.lstm(x)\n",
        "        return self.fc(h_n[-1])\n",
        "\n",
        "model = LSTMClassifier(n_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "dee11132",
      "metadata": {
        "id": "dee11132"
      },
      "outputs": [],
      "source": [
        "def accuracy(logits, target):\n",
        "    return (logits.argmax(1) == target).float().mean().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "932dfc48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "932dfc48",
        "outputId": "c0b934f1-c764-41a4-c066-0470e4613c46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1 | train_acc 0.500 | val_loss 0.6915 | val_acc 0.511\n",
            "Epoch  2 | train_acc 0.544 | val_loss 0.6878 | val_acc 0.556\n",
            "Epoch  3 | train_acc 0.583 | val_loss 0.6838 | val_acc 0.583\n",
            "Epoch  4 | train_acc 0.650 | val_loss 0.6786 | val_acc 0.594\n",
            "Epoch  5 | train_acc 0.628 | val_loss 0.6711 | val_acc 0.589\n",
            "Epoch  6 | train_acc 0.633 | val_loss 0.6589 | val_acc 0.622\n",
            "Epoch  7 | train_acc 0.672 | val_loss 0.6363 | val_acc 0.661\n",
            "Epoch  8 | train_acc 0.711 | val_loss 0.5881 | val_acc 0.722\n",
            "Epoch  9 | train_acc 0.828 | val_loss 0.5021 | val_acc 0.806\n",
            "Epoch 10 | train_acc 0.850 | val_loss 0.3896 | val_acc 0.872\n",
            "Epoch 11 | train_acc 0.889 | val_loss 0.3041 | val_acc 0.883\n",
            "Epoch 12 | train_acc 0.856 | val_loss 0.2788 | val_acc 0.906\n",
            "Epoch 13 | train_acc 0.878 | val_loss 0.2490 | val_acc 0.911\n",
            "Epoch 14 | train_acc 0.894 | val_loss 0.2379 | val_acc 0.906\n",
            "Epoch 15 | train_acc 0.894 | val_loss 0.2472 | val_acc 0.911\n",
            "Epoch 16 | train_acc 0.883 | val_loss 0.2546 | val_acc 0.906\n",
            "Epoch 17 | train_acc 0.906 | val_loss 0.2767 | val_acc 0.889\n",
            "Epoch 18 | train_acc 0.911 | val_loss 0.2527 | val_acc 0.894\n",
            "⏹ Early stopping classifier at epoch 18\n"
          ]
        }
      ],
      "source": [
        "stopper_cls = EarlyStopping(patience=4, mode=\"min\")\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    total_loss, correct = 0., 0\n",
        "    for xb, yb in train_dl:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(xb)\n",
        "        loss = criterion(out, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        correct += (out.argmax(1) == yb).sum().item()\n",
        "    train_acc = correct / len(train_ds)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_correct = 0., 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_dl:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            out = model(xb)\n",
        "            val_loss += criterion(out, yb).item() * xb.size(0)\n",
        "            val_correct += (out.argmax(1) == yb).sum().item()\n",
        "    val_loss /= len(test_ds)\n",
        "    val_acc = val_correct / len(test_ds)\n",
        "    print(f'Epoch {epoch:2d} | train_acc {train_acc:.3f} | val_loss {val_loss:.4f} | val_acc {val_acc:.3f}')\n",
        "\n",
        "    if stopper_cls.step(val_loss):\n",
        "        print(f'⏹ Early stopping classifier at epoch {epoch}')\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d42be3dc",
      "metadata": {
        "id": "d42be3dc"
      },
      "source": [
        "### Surrogate adversarial model (attack_LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0f056a37",
      "metadata": {
        "id": "0f056a37"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Activation(nn.Module):\n",
        "    def __init__(self, kind='identity'):\n",
        "        super().__init__()\n",
        "        if kind == 'identity':\n",
        "            self.act = nn.Identity()\n",
        "        elif kind == 'relu':\n",
        "            self.act = nn.ReLU()\n",
        "        elif kind == 'tanh':\n",
        "            self.act = nn.Tanh()\n",
        "        else:\n",
        "            raise ValueError(f'Unknown activation {kind}')\n",
        "    def forward(self, x):\n",
        "        return self.act(x)\n",
        "\n",
        "class attack_LSTM(nn.Module):\n",
        "    def __init__(self, hidden_dim=64, x_dim=1, activation_type='identity'):\n",
        "        super().__init__()\n",
        "        self.rnn_inp = nn.LSTM(x_dim, hidden_dim, num_layers=3, batch_first=True, dropout=0.4)\n",
        "        self.act = Activation(activation_type)\n",
        "        self.rnn_out = nn.LSTM(hidden_dim, hidden_dim, num_layers=3, batch_first=True, dropout=0.4)\n",
        "        self.fc = nn.Linear(hidden_dim, x_dim)\n",
        "    def forward(self, data):\n",
        "        x, _ = self.rnn_inp(data)\n",
        "        x = self.act(x)\n",
        "        x, _ = self.rnn_out(x)\n",
        "        return self.fc(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2b7ea2c5",
      "metadata": {
        "id": "2b7ea2c5"
      },
      "outputs": [],
      "source": [
        "def accuracy_from_logits(logits, target):\n",
        "    return (logits.argmax(1) == target).float().mean().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f72c8c50",
      "metadata": {
        "id": "f72c8c50"
      },
      "outputs": [],
      "source": [
        "EPS = 0.23\n",
        "def train_surrogate(surr, victim, loader, eps=EPS, epochs=50, lr=1e-4, alpha_l2=1e-3, device='cpu', patience=4):\n",
        "    surr.to(device)\n",
        "    victim.to(device).eval()\n",
        "    opt = torch.optim.Adam(surr.parameters(), lr)\n",
        "    stopper = EarlyStopping(patience=patience, mode=\"max\")\n",
        "    for ep in range(1, epochs+1):\n",
        "        surr.train()\n",
        "        run_vloss, run_acc, n = 0., 0., 0\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            delta = eps * torch.tanh(surr(x))\n",
        "            x_adv = torch.clamp(x + delta, -1, 1)\n",
        "            logits = victim(x_adv)\n",
        "            vloss = F.cross_entropy(logits, y)\n",
        "            acc = (logits.argmax(1) == y).float().mean().item()\n",
        "            reg = alpha_l2 * (delta**2).mean()\n",
        "            loss = -(vloss - reg)\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            run_vloss += vloss.item()*x.size(0)\n",
        "            run_acc += acc * x.size(0)\n",
        "            n += x.size(0)\n",
        "        val_loss = run_vloss / n\n",
        "        print(f'Epoch {ep:02d} | victim‑loss {val_loss:.4f} | acc {run_acc/n:.4f}')\n",
        "        if stopper.step(val_loss):\n",
        "            print(f'⏹ Early stopping at epoch {ep}')\n",
        "            break\n",
        "    torch.save(surr.state_dict(), 'surrogate_maxloss.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "fdfb70b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdfb70b1",
        "outputId": "cae76ed3-078f-4108-eaa4-f6efb8bd3294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | victim‑loss 0.1883 | acc 0.9333\n",
            "Epoch 02 | victim‑loss 0.1885 | acc 0.9333\n",
            "Epoch 03 | victim‑loss 0.1889 | acc 0.9333\n",
            "Epoch 04 | victim‑loss 0.1891 | acc 0.9278\n",
            "Epoch 05 | victim‑loss 0.1894 | acc 0.9278\n",
            "Epoch 06 | victim‑loss 0.1896 | acc 0.9278\n",
            "Epoch 07 | victim‑loss 0.1899 | acc 0.9278\n",
            "Epoch 08 | victim‑loss 0.1903 | acc 0.9278\n",
            "Epoch 09 | victim‑loss 0.1906 | acc 0.9278\n",
            "Epoch 10 | victim‑loss 0.1908 | acc 0.9278\n",
            "Epoch 11 | victim‑loss 0.1912 | acc 0.9278\n",
            "Epoch 12 | victim‑loss 0.1916 | acc 0.9278\n",
            "Epoch 13 | victim‑loss 0.1924 | acc 0.9333\n",
            "Epoch 14 | victim‑loss 0.1925 | acc 0.9333\n",
            "Epoch 15 | victim‑loss 0.1931 | acc 0.9333\n",
            "Epoch 16 | victim‑loss 0.1939 | acc 0.9333\n",
            "Epoch 17 | victim‑loss 0.1945 | acc 0.9333\n",
            "Epoch 18 | victim‑loss 0.1949 | acc 0.9333\n",
            "Epoch 19 | victim‑loss 0.1960 | acc 0.9333\n",
            "Epoch 20 | victim‑loss 0.1972 | acc 0.9333\n",
            "Epoch 21 | victim‑loss 0.1985 | acc 0.9333\n",
            "Epoch 22 | victim‑loss 0.2003 | acc 0.9333\n",
            "Epoch 23 | victim‑loss 0.2018 | acc 0.9222\n",
            "Epoch 24 | victim‑loss 0.2044 | acc 0.9222\n",
            "Epoch 25 | victim‑loss 0.2072 | acc 0.9222\n",
            "Epoch 26 | victim‑loss 0.2113 | acc 0.9056\n",
            "Epoch 27 | victim‑loss 0.2178 | acc 0.9056\n",
            "Epoch 28 | victim‑loss 0.2260 | acc 0.9056\n",
            "Epoch 29 | victim‑loss 0.2370 | acc 0.9000\n",
            "Epoch 30 | victim‑loss 0.2544 | acc 0.8889\n",
            "Epoch 31 | victim‑loss 0.2781 | acc 0.8889\n",
            "Epoch 32 | victim‑loss 0.2979 | acc 0.8889\n",
            "Epoch 33 | victim‑loss 0.3142 | acc 0.8833\n",
            "Epoch 34 | victim‑loss 0.3257 | acc 0.8611\n",
            "Epoch 35 | victim‑loss 0.3347 | acc 0.8667\n",
            "Epoch 36 | victim‑loss 0.3392 | acc 0.8611\n",
            "Epoch 37 | victim‑loss 0.3419 | acc 0.8611\n",
            "Epoch 38 | victim‑loss 0.3439 | acc 0.8611\n",
            "Epoch 39 | victim‑loss 0.3452 | acc 0.8611\n",
            "Epoch 40 | victim‑loss 0.3459 | acc 0.8611\n",
            "Epoch 41 | victim‑loss 0.3465 | acc 0.8611\n",
            "Epoch 42 | victim‑loss 0.3470 | acc 0.8611\n",
            "Epoch 43 | victim‑loss 0.3474 | acc 0.8611\n",
            "Epoch 44 | victim‑loss 0.3476 | acc 0.8611\n",
            "Epoch 45 | victim‑loss 0.3478 | acc 0.8611\n",
            "Epoch 46 | victim‑loss 0.3480 | acc 0.8611\n",
            "Epoch 47 | victim‑loss 0.3481 | acc 0.8611\n",
            "Epoch 48 | victim‑loss 0.3482 | acc 0.8611\n",
            "Epoch 49 | victim‑loss 0.3483 | acc 0.8611\n",
            "Epoch 50 | victim‑loss 0.3484 | acc 0.8611\n",
            "Epoch 51 | victim‑loss 0.3485 | acc 0.8611\n",
            "Epoch 52 | victim‑loss 0.3486 | acc 0.8611\n",
            "Epoch 53 | victim‑loss 0.3486 | acc 0.8611\n",
            "Epoch 54 | victim‑loss 0.3487 | acc 0.8611\n",
            "Epoch 55 | victim‑loss 0.3488 | acc 0.8611\n",
            "Epoch 56 | victim‑loss 0.3488 | acc 0.8611\n",
            "Epoch 57 | victim‑loss 0.3488 | acc 0.8611\n",
            "Epoch 58 | victim‑loss 0.3489 | acc 0.8611\n",
            "Epoch 59 | victim‑loss 0.3489 | acc 0.8611\n",
            "Epoch 60 | victim‑loss 0.3490 | acc 0.8611\n",
            "Epoch 61 | victim‑loss 0.3490 | acc 0.8611\n",
            "Epoch 62 | victim‑loss 0.3490 | acc 0.8611\n",
            "Epoch 63 | victim‑loss 0.3491 | acc 0.8611\n",
            "Epoch 64 | victim‑loss 0.3491 | acc 0.8611\n",
            "Epoch 65 | victim‑loss 0.3491 | acc 0.8611\n",
            "Epoch 66 | victim‑loss 0.3491 | acc 0.8611\n",
            "Epoch 67 | victim‑loss 0.3492 | acc 0.8611\n",
            "Epoch 68 | victim‑loss 0.3492 | acc 0.8611\n",
            "Epoch 69 | victim‑loss 0.3492 | acc 0.8611\n",
            "Epoch 70 | victim‑loss 0.3492 | acc 0.8611\n",
            "Epoch 71 | victim‑loss 0.3492 | acc 0.8611\n",
            "Epoch 72 | victim‑loss 0.3492 | acc 0.8611\n",
            "Epoch 73 | victim‑loss 0.3492 | acc 0.8611\n",
            "Epoch 74 | victim‑loss 0.3493 | acc 0.8611\n",
            "Epoch 75 | victim‑loss 0.3493 | acc 0.8611\n",
            "Epoch 76 | victim‑loss 0.3493 | acc 0.8611\n",
            "Epoch 77 | victim‑loss 0.3493 | acc 0.8611\n",
            "Epoch 78 | victim‑loss 0.3493 | acc 0.8611\n",
            "Epoch 79 | victim‑loss 0.3493 | acc 0.8611\n",
            "⏹ Early stopping at epoch 79\n"
          ]
        }
      ],
      "source": [
        "surrogate_LSTM = attack_LSTM(hidden_dim=64, x_dim=1, activation_type='tanh').to(device)\n",
        "for p in model.parameters():\n",
        "    p.requires_grad_(False)\n",
        "model.eval()\n",
        "train_surrogate(surrogate_LSTM, model, train_dl, eps=EPS, epochs=90, lr=1e-4, alpha_l2=1e-3, device=device, patience=7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fooling rate"
      ],
      "metadata": {
        "id": "pdinjrTRhdK4"
      },
      "id": "pdinjrTRhdK4"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def preds_to_labels(logits: torch.Tensor) -> torch.Tensor:\n",
        "    return logits.argmax(dim=1)\n",
        "\n",
        "def fooling_rate(model, loader, attack, device='cpu'):\n",
        "    model.eval()\n",
        "    fooled, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            preds_orig = preds_to_labels(model(x))\n",
        "    fooled, total = 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        preds_orig = preds_to_labels(model(x).detach())\n",
        "        x_adv = attack(model, x, y)\n",
        "        preds_adv = preds_to_labels(model(x_adv).detach())\n",
        "        fooled += (preds_adv != preds_orig).sum().item()\n",
        "        total += x.size(0)\n",
        "    return fooled / total"
      ],
      "metadata": {
        "id": "2_UM1mf7hh6S"
      },
      "id": "2_UM1mf7hh6S",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FGSM / iFGSM"
      ],
      "metadata": {
        "id": "_EF_3eeThs72"
      },
      "id": "_EF_3eeThs72"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7cab5923",
      "metadata": {
        "id": "7cab5923"
      },
      "outputs": [],
      "source": [
        "class Attack:\n",
        "    def __init__(self, eps: float, clamp=(-1, 1)):\n",
        "        self.eps = eps\n",
        "        self.clamp = clamp\n",
        "\n",
        "class FGSMAttack(Attack):\n",
        "    def __call__(self, model, x, y):\n",
        "        x_req = x.clone().detach().requires_grad_(True)\n",
        "        loss = F.cross_entropy(model(x_req), y)\n",
        "        loss.backward()\n",
        "        delta = self.eps * x_req.grad.sign()\n",
        "        x_adv = torch.clamp(x + delta, *self.clamp)\n",
        "        return x_adv.detach()\n",
        "\n",
        "class iFGSMAttack(Attack):\n",
        "    def __init__(self, eps, n_iter=10, alpha=None, clamp=(-1, 1), rand_init=True):\n",
        "        super().__init__(eps, clamp)\n",
        "        self.n_iter = n_iter\n",
        "        self.alpha = alpha if alpha is not None else 1.25 * eps / n_iter\n",
        "        self.rand_init = rand_init\n",
        "\n",
        "    def _clip(self, x_adv, x_orig):\n",
        "        delta = torch.clamp(x_adv - x_orig, min=-self.eps, max=self.eps)\n",
        "        return torch.clamp(x_orig + delta, *self.clamp)\n",
        "\n",
        "    def __call__(self, model, x, y):\n",
        "        model.eval()\n",
        "        if self.rand_init:\n",
        "            x_adv = x + torch.empty_like(x).uniform_(-self.eps, self.eps)\n",
        "            x_adv = self._clip(x_adv, x).detach()\n",
        "        else:\n",
        "            x_adv = x.clone().detach()\n",
        "\n",
        "        for _ in range(self.n_iter):\n",
        "            x_adv.requires_grad_(True)\n",
        "            loss = F.cross_entropy(model(x_adv), y)\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            grad_sign = x_adv.grad.sign()\n",
        "            x_adv = x_adv + self.alpha * grad_sign\n",
        "            x_adv = self._clip(x_adv, x).detach()\n",
        "        return x_adv\n",
        "\n",
        "class ModelBasedAttack(Attack):\n",
        "    def __init__(self, surrogate, eps, clamp=(-1, 1)):\n",
        "        super().__init__(eps, clamp)\n",
        "        self.surr = surrogate.eval()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(self, model, x, y):\n",
        "        delta = self.eps * torch.tanh(self.surr(x))\n",
        "        return torch.clamp(x + delta, *self.clamp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "42ac3b0e",
      "metadata": {
        "id": "42ac3b0e"
      },
      "outputs": [],
      "source": [
        "fgsm_attack = FGSMAttack(EPS)\n",
        "ifgsm_attack = iFGSMAttack(eps=0.26, alpha=0.1, n_iter=70)\n",
        "model_attack_lstm = ModelBasedAttack(surrogate_LSTM, EPS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Surrogate adversarial model (attack_resCNN)"
      ],
      "metadata": {
        "id": "svR3QMMKh3ND"
      },
      "id": "svR3QMMKh3ND"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ZXhnGDc6IsRR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXhnGDc6IsRR",
        "outputId": "c576d227-f03d-40bf-f829-cd91e2244b7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tsai in /usr/local/lib/python3.11/dist-packages (0.4.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (from tsai) (24.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tsai) (25.0)\n",
            "Requirement already satisfied: fastai>=2.7.18 in /usr/local/lib/python3.11/dist-packages (from tsai) (2.7.19)\n",
            "Requirement already satisfied: pyts>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from tsai) (0.13.0)\n",
            "Requirement already satisfied: imbalanced-learn>=0.12.4 in /usr/local/lib/python3.11/dist-packages (from tsai) (0.13.0)\n",
            "Requirement already satisfied: psutil>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from tsai) (7.0.0)\n",
            "Requirement already satisfied: scikit-learn>=1.5.2 in /usr/local/lib/python3.11/dist-packages (from tsai) (1.6.1)\n",
            "Requirement already satisfied: torch<2.6,>=1.10 in /usr/local/lib/python3.11/dist-packages (from tsai) (2.5.1)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from fastai>=2.7.18->tsai) (0.0.7)\n",
            "Requirement already satisfied: fastcore<1.8,>=1.5.29 in /usr/local/lib/python3.11/dist-packages (from fastai>=2.7.18->tsai) (1.7.29)\n",
            "Requirement already satisfied: torchvision>=0.11 in /usr/local/lib/python3.11/dist-packages (from fastai>=2.7.18->tsai) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from fastai>=2.7.18->tsai) (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from fastai>=2.7.18->tsai) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from fastai>=2.7.18->tsai) (2.32.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from fastai>=2.7.18->tsai) (6.0.2)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.11/dist-packages (from fastai>=2.7.18->tsai) (1.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from fastai>=2.7.18->tsai) (11.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from fastai>=2.7.18->tsai) (1.16.0)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.11/dist-packages (from fastai>=2.7.18->tsai) (3.8.7)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn>=0.12.4->tsai) (2.0.2)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn>=0.12.4->tsai) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn>=0.12.4->tsai) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn>=0.12.4->tsai) (3.6.0)\n",
            "Requirement already satisfied: numba>=0.55.2 in /usr/local/lib/python3.11/dist-packages (from pyts>=0.13.0->tsai) (0.60.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->tsai) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->tsai) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->tsai) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->tsai) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->tsai) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->tsai) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->tsai) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->tsai) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->tsai) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->tsai) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->tsai) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->tsai) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->tsai) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->tsai) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->tsai) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->tsai) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->tsai) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->tsai) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->tsai) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.6,>=1.10->tsai) (1.3.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.55.2->pyts>=0.13.0->tsai) (0.43.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai>=2.7.18->tsai) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai>=2.7.18->tsai) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai>=2.7.18->tsai) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai>=2.7.18->tsai) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai>=2.7.18->tsai) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai>=2.7.18->tsai) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai>=2.7.18->tsai) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai>=2.7.18->tsai) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai>=2.7.18->tsai) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai>=2.7.18->tsai) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai>=2.7.18->tsai) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai>=2.7.18->tsai) (4.67.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai>=2.7.18->tsai) (2.11.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai>=2.7.18->tsai) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai>=2.7.18->tsai) (3.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->fastai>=2.7.18->tsai) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->fastai>=2.7.18->tsai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->fastai>=2.7.18->tsai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->fastai>=2.7.18->tsai) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.6,>=1.10->tsai) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fastai>=2.7.18->tsai) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fastai>=2.7.18->tsai) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fastai>=2.7.18->tsai) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fastai>=2.7.18->tsai) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fastai>=2.7.18->tsai) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fastai>=2.7.18->tsai) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->fastai>=2.7.18->tsai) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->fastai>=2.7.18->tsai) (2025.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4->fastai>=2.7.18->tsai) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai>=2.7.18->tsai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai>=2.7.18->tsai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai>=2.7.18->tsai) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->fastai>=2.7.18->tsai) (1.17.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4->fastai>=2.7.18->tsai) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4->fastai>=2.7.18->tsai) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai>=2.7.18->tsai) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai>=2.7.18->tsai) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai>=2.7.18->tsai) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai>=2.7.18->tsai) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai>=2.7.18->tsai) (7.3.0.post1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4->fastai>=2.7.18->tsai) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai>=2.7.18->tsai) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai>=2.7.18->tsai) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4->fastai>=2.7.18->tsai) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai>=2.7.18->tsai) (0.1.2)\n",
            "\u001b[31mERROR: Invalid requirement: 'torch=2.5.1': Expected end or semicolon (after name and no valid version specifier)\n",
            "    torch=2.5.1\n",
            "         ^\n",
            "Hint: = is not a valid operator. Did you mean == ?\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install tsai\n",
        "!pip install torch=2.5.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "TJ8AvmLrJ9M4",
      "metadata": {
        "id": "TJ8AvmLrJ9M4"
      },
      "outputs": [],
      "source": [
        "from tsai.models.all import ResCNN\n",
        "import inspect\n",
        "\n",
        "class ResCNNModel(nn.Module):\n",
        "    def __init__(self, x_dim=1, output_dim=n_classes,\n",
        "                 activation_type='identity',\n",
        "                 rescnn_kwargs=None):\n",
        "        super().__init__()\n",
        "        self.x_dim = x_dim\n",
        "        rescnn_kwargs = rescnn_kwargs or {}\n",
        "        self.body = ResCNN(c_in=x_dim, c_out=output_dim, **rescnn_kwargs)\n",
        "        self.fin = Activation(activation_type)\n",
        "    def forward(self, x):\n",
        "        if x.ndim == 3 and x.shape[1] != self.x_dim:\n",
        "            x = x.transpose(1, 2)\n",
        "        return self.fin(self.body(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "Er4-JmhmKBEd",
      "metadata": {
        "id": "Er4-JmhmKBEd"
      },
      "outputs": [],
      "source": [
        "class AttackCNN(nn.Module):\n",
        "    def __init__(self, hidden_dim=128, x_dim=1, activation_type='tanh'):\n",
        "        super().__init__()\n",
        "        self.step_cnn = ResCNNModel(x_dim=x_dim, output_dim=hidden_dim, activation_type='identity')\n",
        "        self.fc = nn.Linear(hidden_dim, x_dim)\n",
        "        self.act = Activation(activation_type)\n",
        "    def forward(self, x):\n",
        "        B, L, C = x.shape\n",
        "        x_flat = x.contiguous().view(B * L, 1, C)\n",
        "        h = self.step_cnn(x_flat)\n",
        "        h = h.view(B, L, -1)\n",
        "        return self.fc(self.act(h))\n",
        "\n",
        "surrogate_cnn = AttackCNN().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "dLGgPAuoKMLm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLGgPAuoKMLm",
        "outputId": "471bd8e1-33b9-49fb-959b-3baf4c1ec131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | victim‑loss 0.2058 | acc 0.9222\n",
            "Epoch 02 | victim‑loss 0.2302 | acc 0.9000\n",
            "Epoch 03 | victim‑loss 0.2433 | acc 0.8889\n",
            "Epoch 04 | victim‑loss 0.2616 | acc 0.8889\n",
            "Epoch 05 | victim‑loss 0.2771 | acc 0.8889\n",
            "Epoch 06 | victim‑loss 0.2910 | acc 0.8889\n",
            "Epoch 07 | victim‑loss 0.3022 | acc 0.8889\n",
            "Epoch 08 | victim‑loss 0.3071 | acc 0.8833\n",
            "Epoch 09 | victim‑loss 0.3137 | acc 0.8722\n",
            "Epoch 10 | victim‑loss 0.3178 | acc 0.8778\n",
            "Epoch 11 | victim‑loss 0.3244 | acc 0.8667\n",
            "Epoch 12 | victim‑loss 0.3262 | acc 0.8667\n",
            "Epoch 13 | victim‑loss 0.3295 | acc 0.8611\n",
            "Epoch 14 | victim‑loss 0.3314 | acc 0.8611\n",
            "Epoch 15 | victim‑loss 0.3345 | acc 0.8667\n",
            "Epoch 16 | victim‑loss 0.3329 | acc 0.8667\n",
            "Epoch 17 | victim‑loss 0.3369 | acc 0.8667\n",
            "Epoch 18 | victim‑loss 0.3377 | acc 0.8667\n",
            "Epoch 19 | victim‑loss 0.3401 | acc 0.8611\n",
            "Epoch 20 | victim‑loss 0.3383 | acc 0.8611\n",
            "Epoch 21 | victim‑loss 0.3413 | acc 0.8611\n",
            "Epoch 22 | victim‑loss 0.3419 | acc 0.8611\n",
            "Epoch 23 | victim‑loss 0.3414 | acc 0.8611\n",
            "Epoch 24 | victim‑loss 0.3428 | acc 0.8611\n",
            "Epoch 25 | victim‑loss 0.3432 | acc 0.8611\n",
            "Epoch 26 | victim‑loss 0.3433 | acc 0.8611\n",
            "Epoch 27 | victim‑loss 0.3437 | acc 0.8611\n",
            "Epoch 28 | victim‑loss 0.3441 | acc 0.8611\n",
            "Epoch 29 | victim‑loss 0.3443 | acc 0.8611\n",
            "Epoch 30 | victim‑loss 0.3446 | acc 0.8611\n",
            "Epoch 31 | victim‑loss 0.3453 | acc 0.8611\n",
            "Epoch 32 | victim‑loss 0.3407 | acc 0.8611\n",
            "Epoch 33 | victim‑loss 0.3450 | acc 0.8611\n",
            "Epoch 34 | victim‑loss 0.3457 | acc 0.8611\n",
            "Epoch 35 | victim‑loss 0.3459 | acc 0.8611\n",
            "Epoch 36 | victim‑loss 0.3449 | acc 0.8611\n",
            "Epoch 37 | victim‑loss 0.3461 | acc 0.8611\n",
            "Epoch 38 | victim‑loss 0.3457 | acc 0.8611\n",
            "Epoch 39 | victim‑loss 0.3463 | acc 0.8611\n",
            "Epoch 40 | victim‑loss 0.3462 | acc 0.8611\n",
            "Epoch 41 | victim‑loss 0.3465 | acc 0.8611\n",
            "Epoch 42 | victim‑loss 0.3468 | acc 0.8611\n",
            "Epoch 43 | victim‑loss 0.3467 | acc 0.8611\n",
            "Epoch 44 | victim‑loss 0.3472 | acc 0.8611\n",
            "Epoch 45 | victim‑loss 0.3474 | acc 0.8611\n",
            "Epoch 46 | victim‑loss 0.3466 | acc 0.8611\n",
            "Epoch 47 | victim‑loss 0.3475 | acc 0.8611\n",
            "Epoch 48 | victim‑loss 0.3472 | acc 0.8611\n",
            "Epoch 49 | victim‑loss 0.3474 | acc 0.8611\n",
            "⏹ Early stopping at epoch 49\n"
          ]
        }
      ],
      "source": [
        "train_surrogate(surrogate_cnn, model, train_dl,\n",
        "                eps=EPS, epochs=90, lr=1e-4, alpha_l2=1e-3, device=device, patience=4)\n",
        "\n",
        "model_attack_cnn = ModelBasedAttack(surrogate_cnn, eps=EPS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final compare"
      ],
      "metadata": {
        "id": "Bvem0V2fh_3P"
      },
      "id": "Bvem0V2fh_3P"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "s0BXdVddVqAq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0BXdVddVqAq",
        "outputId": "9908044c-ff95-4948-ac42-c891bf8e187a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fooling rate FGSM  : 0.133\n",
            "Fooling rate iFGSM : 0.156\n",
            "Fooling rate LSTM  : 0.144\n",
            "Fooling rate CNN   : 0.144\n"
          ]
        }
      ],
      "source": [
        "test_loader = test_dl\n",
        "\n",
        "attacks = {'FGSM': fgsm_attack, 'iFGSM': ifgsm_attack, 'LSTM': model_attack_lstm, 'CNN': model_attack_cnn}\n",
        "for name, atk in attacks.items():\n",
        "    rate = fooling_rate(model, test_loader, atk, device=device)\n",
        "    print(f'Fooling rate {name:<6}: {rate:.3f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}